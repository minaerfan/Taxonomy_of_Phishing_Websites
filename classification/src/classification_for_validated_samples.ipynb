{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d629124-35bd-426a-8144-aad076c758a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:31.891992Z",
     "start_time": "2024-06-25T15:34:30.988145Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217aa580-a053-4e13-8684-ffe60f47b389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:33.248478Z",
     "start_time": "2024-06-25T15:34:33.233226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame from the data\n",
    "def create_dataframe(data):\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def domain_age_lessThanOne(create_date, update_date):\n",
    "    if create_date != \"\" and create_date != \"expired\" and not pd.isna(create_date):\n",
    "        age = datetime.strptime(\"2024-04-23\", '%Y-%m-%d') - datetime.strptime(create_date[:10],\n",
    "                                                                              '%Y-%m-%d')\n",
    "        return (age.days // 365) < 1\n",
    "\n",
    "    elif create_date == \"\" and update_date != \"\":\n",
    "        age = datetime.strptime(\"2024-04-23\", '%Y-%m-%d') - datetime.strptime(update_date[:10],\n",
    "                                                                              '%Y-%m-%d')\n",
    "        if age.days < 365:\n",
    "            return None\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    elif create_date == \"expired\":\n",
    "        return True\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:33.750499Z",
     "start_time": "2024-06-25T15:34:33.731269Z"
    }
   },
   "id": "a92da31a22061903",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def binary_to_numeric(value):\n",
    "    if value:\n",
    "        return 1\n",
    "    if not value:\n",
    "        return 0\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:34.257444Z",
     "start_time": "2024-06-25T15:34:34.236846Z"
    }
   },
   "id": "f79c650a5e232ba0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff2a1f7-e937-4acd-bfcc-7f1b144846ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:34.751444Z",
     "start_time": "2024-06-25T15:34:34.737074Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(data, features):\n",
    "    \n",
    "    preprocessed_data = data[features].copy()\n",
    "    preprocessed_data['new_domain'] = None\n",
    "    for index, item in preprocessed_data.iterrows():\n",
    "        new_domain = domain_age_lessThanOne(item['creation_date'], item['updated_date'])\n",
    "        preprocessed_data.loc[index, 'new_domain'] = new_domain\n",
    "        \n",
    "    preprocessed_data = preprocessed_data.drop('creation_date', axis=1)\n",
    "    preprocessed_data = preprocessed_data.drop('updated_date', axis=1)\n",
    "        \n",
    "    # Transform binary values to numerical\n",
    "    preprocessed_data['control_over_dns'] = preprocessed_data['control_over_dns'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['control_over_ssl'] = preprocessed_data['control_over_ssl'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['domain_indexed'] = preprocessed_data['domain_indexed'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_archived'] = preprocessed_data['is_archived'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['known_hosting'] = preprocessed_data['known_hosting'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['new_domain'] = preprocessed_data['new_domain'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_on_root'] = preprocessed_data['is_on_root'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "    preprocessed_data['is_subdomain'] = preprocessed_data['is_subdomain'].astype(float).replace({True: 1.0, False: 0.0})\n",
    "        \n",
    "    # preprocessed_data =preprocessed_data.applymap(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def count_missing_values(data, file_name):\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_values.to_csv(file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:43.787826Z",
     "start_time": "2024-06-25T15:34:43.771029Z"
    }
   },
   "id": "1972842d83aa8e6a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fdbf044-410c-4d6a-817f-d91b56874040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:44.726290Z",
     "start_time": "2024-06-25T15:34:44.711511Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale numerical features using standardization\n",
    "def scale_numerical_features(data, num_features):\n",
    "    scaler = MaxAbsScaler()\n",
    "    data[num_features] = scaler.fit_transform(data[num_features])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0385f52-e76d-4a13-9e50-e2cec38dcd3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:48.515914Z",
     "start_time": "2024-06-25T15:34:48.494471Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(data, num_features):\n",
    "    features_with_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "    features_with_missing_values.sort(key=lambda x: data[x].isnull().sum())\n",
    "\n",
    "    best_params_dict = {}  # Dictionary to store best parameters for each feature\n",
    "\n",
    "    for feature in features_with_missing_values:\n",
    "        complete_data = data.dropna()  # Remove rows with any missing values\n",
    "\n",
    "        data_null = data[data[feature].isnull()] # Rows from the original DataFrame where the values in the column [feature] are null\n",
    "        data_pred = data_null.drop(feature, axis=1) # Test data that we want to predict them the column [feature] \n",
    "\n",
    "        X_train = complete_data.drop(feature, axis=1)\n",
    "        y_train = complete_data[feature]\n",
    "\n",
    "        param_grid = {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"max_depth\": [None, 5, 10],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "        if feature in num_features:\n",
    "            RF_model = RandomForestRegressor()\n",
    "            grid_search = GridSearchCV(RF_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "        else:\n",
    "            RF_model = RandomForestClassifier()\n",
    "            grid_search = GridSearchCV(RF_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params_dict[feature] = grid_search.best_params_  # Save best parameters for the feature\n",
    "        # Print the best hyperparameters for imputing missing values\n",
    "        print(\"Best Parameters:\", grid_search.best_params_)\n",
    "        imputed_values = best_model.predict(data_pred)\n",
    "        data.loc[data[feature].isnull(), feature] = imputed_values\n",
    "        # Calculate accuracy and MSE\n",
    "        best_scoring = grid_search.best_score_\n",
    "        print(f\"Feature: {feature}, Best Score: {best_scoring}\")\n",
    "        \n",
    "\n",
    "    # Save the best parameters dictionary to a file\n",
    "    joblib.dump(best_params_dict, path_prefix + 'best_params_dict.pkl')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e64fc8-a844-42af-a2b0-da7eae5256f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T15:34:49.320378Z",
     "start_time": "2024-06-25T15:34:49.302758Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.base import clone\n",
    "\n",
    "def handle_missing_values_load_bestparams(data, num_features):\n",
    "    features_with_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "    features_with_missing_values.sort(key=lambda x: data[x].isnull().sum())\n",
    "\n",
    "    # Load the best parameters from the pickle file\n",
    "    with open(path_prefix + 'best_params_dict.pkl', 'rb') as f:\n",
    "        best_params_dict = pickle.load(f)\n",
    "\n",
    "    for feature in features_with_missing_values:\n",
    "        complete_data = data.dropna()  # Remove rows with any missing values\n",
    "\n",
    "        data_null = data[data[feature].isnull()]\n",
    "        data_pred = data_null.drop(feature, axis=1)\n",
    "\n",
    "        X_train = complete_data.drop(feature, axis=1)\n",
    "        y_train = complete_data[feature]\n",
    "\n",
    "        if feature in num_features:\n",
    "            RF_model = RandomForestRegressor()\n",
    "        else:\n",
    "            RF_model = RandomForestClassifier()\n",
    "\n",
    "        if feature in best_params_dict:\n",
    "            best_params = best_params_dict[feature] # Get the best parameters for the feature\n",
    "        else:\n",
    "            # Set best_params to default parameters\n",
    "            default_model = clone(RF_model)\n",
    "            default_params = default_model.get_params()\n",
    "            best_params = default_params\n",
    "            \n",
    "        RF_model.set_params(**best_params)  # Set the best parameters for the model\n",
    "\n",
    "        RF_model.fit(X_train, y_train)\n",
    "        imputed_values = RF_model.predict(data_pred)\n",
    "        data.loc[data[feature].isnull(), feature] = imputed_values.astype(float) \n",
    "        # Calculate accuracy and MSE\n",
    "        best_scoring = RF_model.score(X_train, y_train)\n",
    "        print(f\"Feature: {feature}, Best Score: {best_scoring}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "def perform_classification(data, labels, sample_ids):\n",
    "    # Separate the target feature\n",
    "    X = data.copy()\n",
    "    y = labels\n",
    "    mapped_y = target.map({'attackers_domain': 0, 'compromised_domain': 1, 'shared_domain': 2})\n",
    "    \n",
    "    param_grid = {\n",
    "                \"n_estimators\": [100, 200, 500],\n",
    "                \"max_depth\": [None, 5, 10],\n",
    "                \"min_samples_split\": [2, 5, 10]\n",
    "    }\n",
    "    model_to_tune = RandomForestClassifier()\n",
    "    \n",
    "    # Declare the inner and outer cross-validation strategies\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Inner cross-validation for parameter search\n",
    "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv)\n",
    "    \n",
    "    # Outer cross-validation to compute the testing score\n",
    "    outer_confusion_matrices = []\n",
    "    outer_precision_recall_f1 = []\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    sample_id_list = []\n",
    "    \n",
    "    for i, (outer_train_index, outer_test_index) in enumerate(outer_cv.split(X, y)):\n",
    "        X_outer_train, X_outer_test = X.iloc[outer_train_index], X.iloc[outer_test_index]\n",
    "        y_outer_train, y_outer_test = y.iloc[outer_train_index], y.iloc[outer_test_index]\n",
    "        sample_ids_outer_test = sample_ids.iloc[outer_test_index]\n",
    "\n",
    "        model.fit(X_outer_train, y_outer_train)\n",
    "        y_pred = model.predict(X_outer_test)\n",
    "\n",
    "        confusion_matrix_values = confusion_matrix(y_outer_test, y_pred)\n",
    "        outer_confusion_matrices.append(confusion_matrix_values)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_outer_test, y_pred, average=None)\n",
    "        outer_precision_recall_f1.append((precision, recall, f1))\n",
    "\n",
    "        y_true_list.extend(y_outer_test)\n",
    "        y_pred_list.extend(y_pred)\n",
    "        sample_id_list.extend(sample_ids_outer_test)\n",
    "\n",
    "        print(f\"Outer Fold {i+1} Confusion Matrix:\\n{outer_confusion_matrices[-1]}\")\n",
    "        # Print the precision, recall, and F1-score for each class\n",
    "        for j, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "            print(f\"Outer Fold {i+1} Class {j} Precision: {p:.3f}, Recall: {r:.3f}, F1-score: {f:.3f}\")\n",
    "\n",
    "        # Save the predicted values and sample IDs for the current outer fold\n",
    "        fold_data = pd.DataFrame({'sample_id': sample_ids_outer_test, 'actual': y_outer_test, 'predicted': y_pred})\n",
    "        fold_data.to_csv(path_prefix + f'random_forest_predictions_fold{i+1}.csv', index=False)\n",
    "        \n",
    "    # Print the best hyperparameters for classification\n",
    "    print(\"Best Parameters:\", model.best_params_)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model, path_prefix + 'random_forest_model.pkl')\n",
    "    \n",
    "    # Save the model parameters\n",
    "    model_params = model.get_params()\n",
    "    with open(path_prefix + 'random_forest_model_params.txt', 'w') as f:\n",
    "        for param, value in model_params.items():\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "    \n",
    "    # Save the predicted values for all folds\n",
    "    fold_data_all = pd.DataFrame({'actual': y_true_list, 'predicted': y_pred_list})\n",
    "    fold_data_all.to_csv(path_prefix + f'random_forest_predictions_all.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T16:58:41.371617Z",
     "start_time": "2024-06-25T16:58:41.355627Z"
    }
   },
   "id": "1b62131c33cc72cf",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d8fd297-f2db-4588-b200-a1688013e92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T16:58:41.868749Z",
     "start_time": "2024-06-25T16:58:41.854275Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of selected features\n",
    "selected_features = [\n",
    "    'creation_date',\n",
    "    'updated_date',\n",
    "    'control_over_ssl',\n",
    "    'control_over_dns',\n",
    "    'domain_indexed',\n",
    "    'known_hosting',\n",
    "    'is_archived',\n",
    "    'is_on_root',\n",
    "    'is_subdomain',\n",
    "    'between_archives_distance',\n",
    "    'phish_archives_distance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6629d049-b414-431a-ab84-2ce3606d6cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T16:58:42.283896Z",
     "start_time": "2024-06-25T16:58:42.265908Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['between_archives_distance', 'phish_archives_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "130a5bf5-d4dc-4c84-b64a-aeed112ef9c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:40:03.692034Z",
     "start_time": "2024-06-26T04:40:03.669140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path_prefix = '../data/'\n",
    "df = pd.read_csv(path_prefix + 'validated_dataset_for_classification.csv')\n",
    "target = df['verified_category']\n",
    "ids = df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d00114fe-3a52-4044-a5d5-0cee666dd444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:40:17.745517Z",
     "start_time": "2024-06-26T04:40:17.289154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "transformed_data = preprocess_data(df, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c070521-4b59-4487-8468-6f81587a511c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:40:18.519426Z",
     "start_time": "2024-06-26T04:40:18.512445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaled_data = scale_numerical_features(transformed_data, numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# count_missing_values(transformed_data, path_prefix + 'missing_value_count.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:40:20.459230Z",
     "start_time": "2024-06-26T04:40:20.447525Z"
    }
   },
   "id": "5368a5a779770163",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f39f6305-680b-4f89-9912-be2538c42142",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T04:42:16.141981Z",
     "start_time": "2024-06-26T04:40:21.327038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Feature: control_over_ssl, Best Score: 0.8718886965014331\n",
      "Best Parameters: {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Feature: new_domain, Best Score: 0.9798148148148149\n"
     ]
    }
   ],
   "source": [
    "# Handling missing value using RandomForest\n",
    "handled_missed = handle_missing_values(scaled_data, numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['attackers_domain', 'compromised_domain', 'shared_domain'],\n",
      "      dtype=object), array([1376,  106, 3954]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# handled_missed.to_csv(path_prefix + 'handled_missed.csv', index=True)\n",
    "print(np.unique(target, return_counts=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:43:49.720299Z",
     "start_time": "2024-06-26T04:43:49.673703Z"
    }
   },
   "id": "f7428810-b14c-489c-a6fa-e4ad660673f8",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Fold 1 Confusion Matrix:\n",
      "[[274   1   1]\n",
      " [  3  14   4]\n",
      " [  4   3 784]]\n",
      "Outer Fold 1 Class 0 Precision: 0.975, Recall: 0.993, F1-score: 0.984\n",
      "Outer Fold 1 Class 1 Precision: 0.778, Recall: 0.667, F1-score: 0.718\n",
      "Outer Fold 1 Class 2 Precision: 0.994, Recall: 0.991, F1-score: 0.992\n",
      "Outer Fold 2 Confusion Matrix:\n",
      "[[272   2   1]\n",
      " [  3  14   5]\n",
      " [  1   3 786]]\n",
      "Outer Fold 2 Class 0 Precision: 0.986, Recall: 0.989, F1-score: 0.987\n",
      "Outer Fold 2 Class 1 Precision: 0.737, Recall: 0.636, F1-score: 0.683\n",
      "Outer Fold 2 Class 2 Precision: 0.992, Recall: 0.995, F1-score: 0.994\n",
      "Outer Fold 3 Confusion Matrix:\n",
      "[[273   0   2]\n",
      " [  2  17   2]\n",
      " [  0   1 790]]\n",
      "Outer Fold 3 Class 0 Precision: 0.993, Recall: 0.993, F1-score: 0.993\n",
      "Outer Fold 3 Class 1 Precision: 0.944, Recall: 0.810, F1-score: 0.872\n",
      "Outer Fold 3 Class 2 Precision: 0.995, Recall: 0.999, F1-score: 0.997\n",
      "Outer Fold 4 Confusion Matrix:\n",
      "[[271   2   2]\n",
      " [  2  19   0]\n",
      " [  2   4 785]]\n",
      "Outer Fold 4 Class 0 Precision: 0.985, Recall: 0.985, F1-score: 0.985\n",
      "Outer Fold 4 Class 1 Precision: 0.760, Recall: 0.905, F1-score: 0.826\n",
      "Outer Fold 4 Class 2 Precision: 0.997, Recall: 0.992, F1-score: 0.995\n",
      "Outer Fold 5 Confusion Matrix:\n",
      "[[271   2   2]\n",
      " [  4  14   3]\n",
      " [  2   3 786]]\n",
      "Outer Fold 5 Class 0 Precision: 0.978, Recall: 0.985, F1-score: 0.982\n",
      "Outer Fold 5 Class 1 Precision: 0.737, Recall: 0.667, F1-score: 0.700\n",
      "Outer Fold 5 Class 2 Precision: 0.994, Recall: 0.994, F1-score: 0.994\n",
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "perform_classification(handled_missed, target, ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-26T04:47:24.822617Z",
     "start_time": "2024-06-26T04:43:53.310640Z"
    }
   },
   "id": "ca464098bf1fd65e",
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
